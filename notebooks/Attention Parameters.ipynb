{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee9ce36-124a-4f5f-b983-e1afc5722c75",
   "metadata": {},
   "source": [
    "In this notebook I want to play around with the parameters of a transformer encoder to get a better understanding of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aad8881-2500-4bd5-a8ee-8f95e740e01b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849d6cc-8e9f-48ca-9dca-7e4e3dd8ea62",
   "metadata": {},
   "source": [
    "# Token Embedding Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd3f83-d059-44a5-b260-03502bbcfd1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here I want to see if I can find some resons why the BERT<sub>BASE</sub> model chosses a token embedding dimension $H$ (in the BERT paper) of specifically $H=768$. The reason I am interested in this number to begin with is the study of the attention head count hyperparameter $A$. If you want to split the embedding vector equally between all the attention heads but still want a large number of possible attention head counts $A$, then the embedding space dimension should be chosen to have a large number of integer divisors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031714af-dac3-4483-ba1a-617304819fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "H = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df59dbc-78ff-40f7-842c-ddcde926d141",
   "metadata": {},
   "source": [
    "First let's determine the prime number decomposition of the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa467bd-e2b6-4387-9c28-7c1d778d3f61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 8, 3: 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy.ntheory import factorint\n",
    "\n",
    "factorint(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d5254-c992-43e1-98bf-235ad9a4267e",
   "metadata": {
    "tags": []
   },
   "source": [
    "That's a bit underwhelming, it's just $3 \\cdot 256$, three thirds of the $1024$ in the BERT<sub>LARGE</sub> model. Then again powers of two naturally have many divisors. Let's see what divisors or possible values for $A$ we are working with. In that, we limit ourselves to realistic values from $A=1$ (single-head attention) to $A=24$ (1.5 times the value as in BERT<sub>LARGE</sub>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ebac8f-2ec8-442f-995d-233b77125690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 possible values for A: [1, 2, 4, 8, 16, 3, 6, 12, 24]\n"
     ]
    }
   ],
   "source": [
    "from sympy.ntheory import divisors\n",
    "\n",
    "A_max=24\n",
    "A_possible = [n for n in divisors(H, generator=True) if n <= A_max]\n",
    "\n",
    "print(f\"Found {len(A_possible)} possible values for A: {A_possible}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35556737-3e60-4fd5-abef-b33f17c7a6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
